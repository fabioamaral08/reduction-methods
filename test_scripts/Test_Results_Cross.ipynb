{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../src') \n",
    "import numpy as np\n",
    "#for some reason, matplotlib crashes without these lines\n",
    "a = np.zeros((5,5))\n",
    "a@a\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import get_mesh_vtk\n",
    "from utils import get_data\n",
    "from utils import strip_cross, reconstruct_cross\n",
    "from KPCA import *\n",
    "import Autoencoder\n",
    "# from torchsummary import summary\n",
    "# from Autoencoder import Autoencoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import calc_energy, torch2np\n",
    "import os\n",
    "# from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the meshgrid for computing the energy\n",
    "\n",
    "vtk_file = '../npz_data/Dados-N0.vtk'\n",
    "x, y = get_mesh_vtk(vtk_file)\n",
    "cut = 13 # removes the influence of inflow/outflow\n",
    "dx = x[1:] - x[:-1]\n",
    "dy = y[1:] - y[:-1]\n",
    "DX, DY = np.meshgrid(dx,dy)\n",
    "DX = strip_cross(DX[...,None,None], cut).squeeze()\n",
    "DY = strip_cross(DY[...,None,None], cut).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Data reading\n",
    "parameters = [\n",
    "        (0.01,2.5,0.11111),\n",
    "        (0.01,3.5,0.11111),\n",
    "        (0.01,5.0,0.33333),\n",
    "        (0.01,7.0,0.11111),\n",
    "        (0.01,7.5,0.33333),\n",
    "]\n",
    "# Parameters:\n",
    "# Re = 1\n",
    "# Wi = 7.5\n",
    "# beta = 0.6\n",
    "Re ,Wi ,beta = parameters[4]\n",
    "alpha = (1-beta)/(Re*Wi)\n",
    "\n",
    "# type of simulation\n",
    "case = 'cross'\n",
    "#read file\n",
    "X, Xmean = get_data(Re,Wi,beta, case, n_data= -2, dir_path='../npz_data')\n",
    "print('Data shape: ',X.shape)\n",
    "\n",
    "Nt = X.shape[1] # number of snapshots\n",
    "q = X.reshape((181,181,5,-1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_data = strip_cross(q, cut)\n",
    "X_data = np.moveaxis(X_data,[0,2],[2,0]) # (Nx, Nc, Nt) -> (Nt, Nc, Nx)\n",
    "\n",
    "# convert data\n",
    "X_torch = torch.from_numpy(X_data)\n",
    "X = torch2np(X_torch)\n",
    "\n",
    "#normalize data inside autoencoder\n",
    "lower_bound = torch.from_numpy(X_data.min(axis = (0,2)).reshape((1,5,1))).float().to(device)\n",
    "upper_bound = torch.from_numpy(X_data.max(axis = (0,2)).reshape((1,5,1))).float().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN part (Variational AE)\n",
    "latent_dim = 3\n",
    "autoencoder = Autoencoder.VariationalAutoencoderModule(n_input= X_torch.shape[-1], latent_dim = latent_dim, max_in=upper_bound, min_in=lower_bound).to(device)\n",
    "autoencoder.load_state_dict(torch.load(f'../Models/VAE_CrossPred_Latent_{latent_dim}/Re{Re:g}_Wi{Wi:g}_beta{beta:g}/best_autoencoder',map_location=torch.device('cpu')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN part (Standard AE)\n",
    "latent_dim = 3\n",
    "autoencoder = Autoencoder.AutoencoderModule(n_input= X_torch.shape[-1], latent_dim = latent_dim, max_in=upper_bound, min_in=lower_bound).to(device)\n",
    "autoencoder.load_state_dict(torch.load(f'../Models/Dense_CrossPred_Latent_{latent_dim}/Re{Re:g}_Wi{Wi:g}_beta{beta:g}/best_autoencoder',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test\n",
    "with torch.no_grad():\n",
    "    X_ae_torch,_,_ = autoencoder(X_torch.float()) # Variational\n",
    "    # X_ae_torch = autoencoder(X_torch.float()) # Standard\n",
    "X_ae = torch2np(X_ae_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 1\n",
    "ncomp = latent_dim\n",
    "\n",
    "# PCA\n",
    "Qtest, _, _, _ = kpca(X.T, n_components=ncomp, kernel='linear', gamma=alpha, norm='DIV', eps = None) \n",
    "\n",
    "Q2 = np.concatenate([np.ones((X.shape[1],1))] + [Qtest**(k+1) for k in range(degree)], axis=1)\n",
    "R, _, _, _ = np.linalg.lstsq(Q2, X.T, rcond=None)\n",
    "\n",
    "X_linear = R.T@Q2.T\n",
    "\n",
    "# KPCA\n",
    "Qtest, _, _, _ = kpca(X.T, n_components=ncomp, kernel='oldroyd', gamma=alpha, norm='DIV', eps = None, dx = DX[None,:], dy = DY[None,:]) \n",
    "\n",
    "Q2 = np.concatenate([np.ones((X.shape[1],1))] + [Qtest**(k+1) for k in range(degree)], axis=1)\n",
    "R, _, _, _ = np.linalg.lstsq(Q2, X.T, rcond=None)\n",
    "\n",
    "X_oldroyd = R.T@Q2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy From data\n",
    "elastic, kinetic, total = calc_energy(X,Wi,beta,Re, dx = DX[:,None], dy = DY[:,None])\n",
    "# Energy From PCA\n",
    "elastic_linear, kinetic_linear, total_linear = calc_energy(X_linear,Wi,beta,Re,dx = DX[:,None], dy = DY[:,None])\n",
    "# Energy From KPCA\n",
    "elastic_oldroyd, kinetic_oldroyd, total_oldroyd = calc_energy(X_oldroyd,Wi,beta,Re,dx = DX[:,None], dy = DY[:,None])\n",
    "# # Energy From Autoencoder\n",
    "elastic_ae, kinetic_ae, total_ae = calc_energy(X_ae,Wi,beta,Re,dx = DX[:,None], dy = DY[:,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ini = 0\n",
    "fim = -1\n",
    "lw = 2\n",
    "plt.plot(total[ini:fim], label = 'SIMULATION',color='k', lw = lw)\n",
    "plt.plot(total_linear[ini:fim],'--', label = 'PCA', lw = lw)\n",
    "plt.plot(total_oldroyd[ini:fim],'--', label = 'KPCA')\n",
    "plt.plot(total_ae[ini:fim],'--', label = 'VAE', lw = lw)\n",
    "plt.title(f'Re: {Re:g}, Wi: {Wi:g}, $\\\\beta$: {beta:g}')\n",
    "plt.legend()\n",
    "\n",
    "plt.axvline(len(total) - 100,color = 'k', linestyle='dashed')\n",
    "# plt.savefig(f'{imgdir}/Re{Re:g}_Wi{Wi:g}_beta{beta:g}.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
